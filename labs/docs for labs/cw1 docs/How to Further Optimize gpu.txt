Let's go over your code and explain why the GPU is so much faster than the CPU in your implementation. I'll also highlight key parts of your code that contribute to the performance difference.

### Main Reasons the GPU is Faster

1. **Parallelism**:
   - The **CPU version** processes the text in a **sequential manner**. For each word search, the CPU checks each character in the text one by one to see if it matches the token. Even though CPUs are fast, they are limited to a few cores (typically between 4 and 16 cores) and execute a series of instructions in order.
   - The **GPU version**, on the other hand, processes the text **in parallel**. Thousands of threads can be launched at once to check different portions of the text simultaneously. Each thread compares a part of the text with the search token independently, which allows the GPU to search through large amounts of text much faster than the CPU.

2. **Workload Distribution**:
   - In the GPU kernel (`calc_token_occurrences_kernel`), each thread is responsible for comparing the token to a different part of the text. The total number of comparisons that need to be done is divided among hundreds or thousands of threads, making each thread's workload smaller and faster to complete.
   - In contrast, the CPU version (`calc_token_occurrences` function) processes the entire dataset with a single thread, which has to handle every comparison one at a time. This creates a bottleneck because the CPU core cannot distribute the workload as effectively.

3. **Efficient Memory Access**:
   - In your GPU implementation, you are using **global memory** to store the text data (`d_data`), and each thread accesses different parts of this memory. Even though global memory accesses on a GPU can be slower than other types of memory (like shared memory), the fact that many threads can access memory in parallel helps mitigate this bottleneck.
   - In the CPU implementation, memory access is more straightforward but sequential. It fetches data one piece at a time and processes it in a loop, which introduces latency that builds up over time.

4. **Hardware Specialization**:
   - GPUs are designed to excel at tasks that can be parallelized, like the kind of text searching you're doing. GPUs have many cores, each capable of handling thousands of lightweight threads, whereas CPUs are optimized for a smaller number of heavier threads.
   - For tasks like searching a word in a massive text, the CPU must handle each comparison one after the other, while the GPU can simultaneously run many threads to check different parts of the text.

### Breakdown of Your Code

#### CPU Implementation (`main.cpp`)

- In the CPU version, the text is read into a `std::vector<char>` using the `read_file` function.
- The `calc_token_occurrences` function is responsible for counting occurrences of the token:
  - It checks each character in the text sequentially, comparing the current substring with the token.
  - After a match is found, it verifies that the prefix and suffix are non-letter characters to ensure that the match is a complete word.
  - This process happens for **every character in the file**, one after another.

The main bottleneck is that this is all done in a single thread. The CPU must handle the entire dataset, checking each character one at a time, which makes it relatively slow.

#### GPU Implementation (`main.cu`)

- In the GPU version, you first allocate memory on the GPU for the text and the search token.
- The `calc_token_occurrences_kernel` is responsible for counting occurrences of the token:
  - Each thread (running in parallel) is assigned a portion of the text to check.
  - The threads run the `gpu_strncmp` function to compare the token with the corresponding substring in the text.
  - Threads that find a match also check the prefix and suffix characters, similar to the CPU version, to ensure it’s a valid standalone word.
  - If a match is found, `atomicAdd(numOccurrences, 1)` is called to increment the count in a thread-safe manner.

Here, **each thread processes a different portion of the text simultaneously**, which means that instead of one core checking the entire text, hundreds or thousands of threads are working at once. This parallelism allows the GPU to find all occurrences much faster than the CPU.

#### Why GPU is Faster

1. **Thread Parallelism**: Each thread checks a different part of the text, so if you have 1,000 threads, 1,000 comparisons happen simultaneously. In contrast, the CPU can only do one comparison at a time.

2. **Efficient Task Distribution**: The workload is split among many GPU cores, each handling a small part of the file, while the CPU has to handle the whole workload sequentially.

3. **Memory Latency Hiding**: While one thread waits for memory to be fetched, other threads continue processing. This ability to hide memory access latencies is another reason GPUs can outperform CPUs on such tasks.

### How You Can Further Optimize

1. **Shared Memory**: If you’re not already doing so, you could explore using **shared memory** in your kernel for even faster memory access. Shared memory is much faster than global memory and is shared between threads in a block.

2. **Block Size Tuning**: You could experiment with different block sizes to see how they impact performance. The choice of block size can affect how well the GPU resources are utilized.

3. **Coalesced Memory Access**: Ensuring that threads access contiguous memory locations can further improve performance. This is known as **coalesced memory access** and can minimize the number of global memory accesses.

Would you like further clarification or help with any other part of the code or optimizations?